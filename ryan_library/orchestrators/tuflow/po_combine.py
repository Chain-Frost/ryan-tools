"""
PO Combination Utilities.

This module provides the core logic for combining "PO" (Plot Output) CSV files generated by TUFLOW simulations.
It leverages the `ProcessorCollection` to process files in parallel, normalize locations, and filter data types.
The combined results can be exported to Excel or Parquet formats.
"""

from collections.abc import Collection
from pathlib import Path
from typing import Literal

import pandas as pd
from loguru import logger

from ryan_library.functions.tuflow.tuflow_common import collect_files, process_files_in_parallel
from ryan_library.processors.tuflow.base_processor import BaseProcessor
from ryan_library.processors.tuflow.processor_collection import ProcessorCollection
from ryan_library.functions.file_utils import ensure_output_directory
from ryan_library.functions.misc_functions import ExcelExporter
from ryan_library.classes.suffixes_and_dtypes import SuffixesConfig
from ryan_library.functions.loguru_helpers import setup_logger
from ryan_library.functions.tuflow.wrapper_helpers import normalize_data_types, warn_on_invalid_types

DEFAULT_DATA_TYPES: tuple[str, ...] = ("PO",)
ACCEPTED_DATA_TYPES: frozenset[str] = frozenset(DEFAULT_DATA_TYPES)


def main_processing(
    paths_to_process: list[Path],
    include_data_types: list[str] | None = None,
    console_log_level: str = "INFO",
    locations_to_include: Collection[str] | None = None,
    export_mode: Literal["excel", "parquet", "both"] = "excel",
) -> None:
    """
    Generate merged PO data and export the results.

    This function orchestrates the finding, reading, processing, and combining of TUFLOW PO CSV files.
    It supports filtering by specific locations and exporting the final combined dataset.

    Args:
        paths_to_process: A list of directory paths to search for PO files.
        include_data_types: A list of data types to include (e.g., ["PO"]). Defaults to DEFAULT_DATA_TYPES.
        console_log_level: logging level for the console output (e.g., "INFO", "DEBUG").
        locations_to_include: A collection of location names (strings) to keep. If None, all locations are kept.
                              Column matching is case-insensitive.
        export_mode: The format for the output file(s): "excel", "parquet", or "both".
    """

    # validate and normalize requested data types against the accepted set
    requested_types, invalid_types = normalize_data_types(
        requested=include_data_types,
        default=DEFAULT_DATA_TYPES,
        accepted=ACCEPTED_DATA_TYPES,
    )

    # normalize location filters to a frozenset for efficient lookup
    normalized_locations: frozenset[str] = BaseProcessor.normalize_locations(locations=locations_to_include)

    # Setup a fresh logger instance for this processing run
    with setup_logger(console_log_level=console_log_level) as log_queue:
        # separate warning for invalid types if any were dropped
        warn_on_invalid_types(
            invalid_types=invalid_types,
            accepted_types=ACCEPTED_DATA_TYPES,
            context="PO combination",
        )

        # Collect all relevant files from the target directories
        csv_file_list: list[Path] = collect_files(
            paths_to_process=paths_to_process,
            include_data_types=requested_types,
            suffixes_config=SuffixesConfig.get_instance(),
        )
        if not csv_file_list:
            warn_on_invalid_types(
                invalid_types=invalid_types,
                accepted_types=ACCEPTED_DATA_TYPES,
                context="PO combination completed",
            )
            logger.info("No valid files found to process.")
            return

        # Process the collected files using parallel execution
        results_set: ProcessorCollection = process_files_in_parallel(
            file_list=csv_file_list,
            log_queue=log_queue,
            log_level=console_log_level,
            entity_filters=normalized_locations if normalized_locations else None,
        )

        # Export the combined results
        export_results(results=results_set, export_mode=export_mode)
        logger.info("End of PO results combination processing")

        # Re-issue invalid type warnings at the end so they aren't missed in log scroll
        warn_on_invalid_types(
            invalid_types=invalid_types,
            accepted_types=ACCEPTED_DATA_TYPES,
            context="PO combination completed",
        )


def export_results(*, results: ProcessorCollection, export_mode: Literal["excel", "parquet", "both"] = "excel") -> None:
    """
    Export combined DataFrames according to the requested mode.

    Merges all processed individual DataFrames into a single aggregate DataFrame and saves it.

    Args:
        results: The ProcessorCollection containing the processed data.
        export_mode: The desired output format ("excel", "parquet", "both").
    """
    if not results.processors:
        logger.warning("No results to export.")
        return

    # Combine all individual processor results into one large DataFrame
    # po_combine() is a specific method on ProcessorCollection for PO-type data
    combined_df: pd.DataFrame = results.po_combine()
    if combined_df.empty:
        logger.warning("No combined data found. Skipping export.")
        return

    ensure_output_directory(output_dir=Path.cwd())
    exporter = ExcelExporter()
    exporter.save_to_excel(
        data_frame=combined_df,
        file_name_prefix="combined_PO",
        sheet_name="combined_PO",
        output_directory=Path.cwd(),
        export_mode=export_mode,
        parquet_compression="gzip",
    )
