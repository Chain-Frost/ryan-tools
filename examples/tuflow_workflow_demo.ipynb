{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TUFLOW Workflow Demo\n",
    "\n",
    "This notebook demonstrates how to use `ryan-tools` to load, process, and combine TUFLOW results interactively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure ryan-tools is in your path if you haven't installed it as a package\n",
    "# sys.path.append('path/to/ryan-tools')\n",
    "\n",
    "from ryan_library.functions.tuflow.notebook_helpers import load_tuflow_data\n",
    "from ryan_library.processors.tuflow.processor_collection import ProcessorCollection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define Paths and Data Types\n",
    "\n",
    "Specify the directories containing your TUFLOW results and the data types you want to load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update this path to point to your data\n",
    "data_path = Path(\"E:/Project/Results/TUFLOW\")\n",
    "paths_to_process = [data_path]\n",
    "\n",
    "# List required data types (suffixes)\n",
    "# e.g. Q (Flow), V (Velocity), H (Water Level), POMM (Peak of Max/Means)\n",
    "data_types = [\"Q\", \"H\", \"V\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data\n",
    "\n",
    "Use the `load_tuflow_data` helper to scan for files and process them in parallel. \n",
    "This returns a `ProcessorCollection` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = load_tuflow_data(\n",
    "    paths=paths_to_process,\n",
    "    data_types=data_types,\n",
    "    parallel=True,       # Set to False if debugging or for small datasets\n",
    "    log_level=\"INFO\"     # \"DEBUG\" for more verbose output\n",
    ")\n",
    "\n",
    "print(f\"Loaded {len(collection.processors)} files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Filter Locations (Optional)\n",
    "\n",
    "You can filter the loaded data to only include specific culvert or node IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collection.filter_locations([\"Culvert_001\", \"Culvert_002\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Combine Results\n",
    "\n",
    "Combine the loaded data into a single Pandas DataFrame. \n",
    "The method used depends on the data format (Timeseries vs Maximums)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper to combine 1D timeseries data (Q, V, H, etc.)\n",
    "# This merges static attributes (from EOF/Chan files if loaded) and calculates HW/D\n",
    "timeseries_df = collection.combine_1d_timeseries()\n",
    "\n",
    "if not timeseries_df.empty:\n",
    "    display(timeseries_df.head())\n",
    "else:\n",
    "    print(\"No timeseries data found or combined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Caching & Persistence\n",
    "\n",
    "For large datasets, re-scanning and loading files can be slow. You can save the processed collection to disk to resume work later.\n",
    "The generic `save()` and `load()` methods default to using a single HDF5 file, which is fast and convenient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option A: Save as HDF5 (Single file, fast, recommended)\n",
    "collection.save(\"processed_data.h5\")\n",
    "print(f\"Collection saved to processed_data.h5\")\n",
    "\n",
    "# Option B: Save as directory of Parquet files (Good for debugging)\n",
    "# collection.save(\"processed_cache\", format=\"parquet\")\n",
    "# print(f\"Collection saved to processed_cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume from HDF5\n",
    "resumed_collection = ProcessorCollection.load(\"processed_data.h5\")\n",
    "print(f\"Resumed {len(resumed_collection.processors)} processors from file.\")\n",
    "\n",
    "# Resume from Directory\n",
    "# resumed_collection_parquet = ProcessorCollection.load(\"processed_cache\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Advanced Analysis: Mean Max Hydrographs\n",
    "\n",
    "Identify the 'critical' mean simulation for each AEP and plot the hydrographs.\n",
    "This requires both Maximums (to find the mean) and Timeseries (to plot) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ryan_library.functions.tuflow.notebook_helpers import get_critical_hydrographs, plot_hydrographs\n",
    "\n",
    "# Ensure we have a clean copy for analysis if we plan to mutate\n",
    "analysis_collection = collection.copy()\n",
    "\n",
    "# Identify critical hydrographs (based on Flow 'Q')\n",
    "critical_flows = get_critical_hydrographs(analysis_collection, metric=\"Q\")\n",
    "\n",
    "# Plot the results\n",
    "if critical_flows:\n",
    "    plot_hydrographs(critical_flows, title=\"Mean Critical Flow Hydrographs\")\n",
    "else:\n",
    "    print(\"No critical hydrographs found (ensure you loaded Timeseries AND Maximums data).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Custom Analysis\n",
    "\n",
    "Now you have a standard Pandas DataFrame to use for any other plotting or analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not timeseries_df.empty:\n",
    "    # Example: Plot Max Q by AEP for a specific channel\n",
    "    # subset = timeseries_df[timeseries_df[\"Chan ID\"] == \"Example_Culvert\"]\n",
    "    # subset.plot(x=\"aep_numeric\", y=\"Q\", kind=\"scatter\")\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
